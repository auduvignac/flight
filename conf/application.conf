app {
  # "Local" | "Hadoop" (surchargable via APP_ENV)
  env = ${?APP_ENV}
  env = "Local"
  debug = false

  # chemin racine du projet dans ta machine ou dans le conteneur
  local.root = "/app"
  hadoop.root = "hdfs:///projects/flight"
  persist.enabled = false

  input {
    # Donn√©es brutes (en local ou HDFS)
    flights.dir = ${app.local.root}"/data/flights"
    weather.dir = ${app.local.root}"/data/weather"
    mapping     = ${app.local.root}"/data/wban_airport_timezone.csv"
    months_f    = ["201201"]
    months_w    = ["201201"]
  }

  output {
    # En local on remplace dbfs:/delta/... par un dossier delta/
    analysis.dir = ${app.local.root}"/delta/analysis"
    delta.base.bronze = ${app.local.root}"/delta/bronze"
    delta.base.silver = ${app.local.root}"/delta/silver"
    delta.base.gold   = ${app.local.root}"/delta/gold"
  }

  # Profil Hadoop
  hadoop {
    input.flights.dir = ${app.hadoop.root}"/data/flights"
    input.weather.dir = ${app.hadoop.root}"/data/weather"
    input.mapping     = ${app.hadoop.root}"/data/wban_airport_timezone.csv"

    output.analysis = ${app.hadoop.root}"/delta/analysis"
    output.delta.base.bronze = ${app.hadoop.root}"/delta/bronze"
    output.delta.base.silver = ${app.hadoop.root}"/delta/silver"
    output.delta.base.gold   = ${app.hadoop.root}"/delta/gold"
  }

  params {
    thMinutes = 60
    missingness.threshold = 0.60
  }
}

spark {
  master = ${?SPARK_MASTER_URL}
  master = "local[*]"
  appName = "flight"

  sql.extensions = "io.delta.sql.DeltaSparkSessionExtension"
  catalogClass   = "org.apache.spark.sql.delta.catalog.DeltaCatalog"

  conf = {
    "spark.sql.adaptive.enabled" = "true"
    "spark.sql.adaptive.skewJoin.enabled" = "true"
    "spark.sql.files.maxPartitionBytes" = "134217728"
  }
}