name: CI/CD - Flight Project (Optimized with Dataset Management)

on:
  push:
    branches: [main, feature/results-export]
  pull_request:
  workflow_dispatch:

jobs:
  # ============================================
  # Ã‰tape 0 â€” PrÃ©paration du Dataset
  # ============================================
  prepare-dataset:
    name: ðŸ“¦ Prepare Dataset
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - uses: actions/checkout@v4
      - name: Ensure unzip and wget are available
        run: sudo apt-get update -y && sudo apt-get install -y unzip wget curl
      - name: Run dataset preparation script
        run: |
          chmod +x get-data.sh
          ./get-data.sh

      - name: Upload dataset artifact
        uses: actions/upload-artifact@v4
        with:
          name: dataset
          path: data/

  # ============================================
  # Ã‰tape 1 â€” Build Scala & Quality Checks
  # ============================================
  scala-build:
    name: ðŸ§© Scala Build & Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: '11'

      - name: Setup SBT
        uses: coursier/setup-action@v1
        with:
          apps: sbt

      - name: Cache SBT and Ivy
        uses: actions/cache@v4
        with:
          path: |
            ~/.ivy2/cache
            ~/.sbt
            ~/.cache/coursier
          key: ${{ runner.os }}-sbt-${{ hashFiles('**/build.sbt') }}
          restore-keys: |
            ${{ runner.os }}-sbt-

      - name: Clean corrupted Ivy cache
        run: find ~/.ivy2/cache -name "ivydata-*.properties" -delete || true

      # VÃ©rification du style de code
      - name: Check Scalafmt formatting
        run: sbt scalafmtCheckAll

      # VÃ©rification des rÃ¨gles Scalafix
      - name: Check Scalafix rules
        run: sbt "scalafix --check"

      # Compilation stricte
      - name: Compile project
        run: sbt -warn compile

      # Tests unitaires
      - name: Run tests
        run: sbt test

      # GÃ©nÃ©ration du JAR pour l'Ã©tape Docker
      - name: Build Assembly JAR
        run: sbt clean assembly

      - name: Upload JAR artifact
        uses: actions/upload-artifact@v4
        with:
          name: flight-assembly
          path: target/scala-2.12/flight-assembly.jar

  # ============================================
  # Ã‰tape 2 â€” Build Docker image
  # ============================================
  docker-build:
    name: ðŸ³ Build & Push Runtime Docker Image
    needs: scala-build
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download built JAR
        uses: actions/download-artifact@v4
        with:
          name: flight-assembly
          path: .

      - name: Log in to GHCR
        run: echo ${{ secrets.CR_PAT }} | docker login ghcr.io -u auduvignac --password-stdin

      - name: Build and Push Docker image
        run: |
          IMAGE_NAME=ghcr.io/auduvignac/flight:latest
          docker build -f Dockerfile.custom -t $IMAGE_NAME .
          docker push $IMAGE_NAME

  # ============================================
  # Ã‰tape 3 â€” Bronze Stage
  # ============================================
  bronze:
    name: ðŸŸ¤ Bronze - Ingestion & Enrichment
    needs: [prepare-dataset, docker-build]
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/auduvignac/flight:latest
    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v4

      - name: Download dataset
        uses: actions/download-artifact@v4
        with:
          name: dataset
          path: /app/data/

      - name: Run Bronze stage
        run: ./run-app.sh --stage=bronze --local --persist

      - name: Upload Bronze analysis
        uses: actions/upload-artifact@v4
        with:
          name: analysis-bronze
          path: analysis/

      - name: Upload Delta Bronze
        uses: actions/upload-artifact@v4
        with:
          name: delta-bronze
          path: /app/delta/

  # ============================================
  # Ã‰tape 3bis â€” Full Pipeline (All Stages)
  # ============================================
  # full-pipeline:
  #   name: ðŸŒˆ Full Pipeline - End-to-End Run
  #   needs: [prepare-dataset, docker-build]
  #   runs-on: ubuntu-latest
  #   container:
  #     image: ghcr.io/auduvignac/flight:latest
  #   timeout-minutes: 120

  #   strategy:
  #     fail-fast: false
  #     matrix:
  #       exp:
  #         [
  #           # === Baseline
  #           { ds: "D2", th: 60, origin: 0, dest: 0, tag: "Baseline_D2_th60_noWeather" },

  #           # === Ã‰tude 1 : Origine seule
  #           { ds: "D2", th: 60, origin: 1, dest: 0, tag: "S1_origin_1h_D2_th60" },
  #           { ds: "D2", th: 60, origin: 3, dest: 0, tag: "S1_origin_3h_D2_th60" },
  #           { ds: "D2", th: 60, origin: 5, dest: 0, tag: "S1_origin_5h_D2_th60" },
  #           { ds: "D2", th: 60, origin: 7, dest: 0, tag: "S1_origin_7h_D2_th60" },
  #           { ds: "D2", th: 60, origin: 9, dest: 0, tag: "S1_origin_9h_D2_th60" },
  #           { ds: "D2", th: 60, origin: 11, dest: 0, tag: "S1_origin_11h_D2_th60" },

  #           # === Ã‰tude 1 : Destination seule
  #           { ds: "D2", th: 60, origin: 0, dest: 1, tag: "S1_dest_1h_D2_th60" },
  #           { ds: "D2", th: 60, origin: 0, dest: 3, tag: "S1_dest_3h_D2_th60" },
  #           { ds: "D2", th: 60, origin: 0, dest: 5, tag: "S1_dest_5h_D2_th60" },
  #           { ds: "D2", th: 60, origin: 0, dest: 7, tag: "S1_dest_7h_D2_th60" },
  #           { ds: "D2", th: 60, origin: 0, dest: 9, tag: "S1_dest_9h_D2_th60" },
  #           { ds: "D2", th: 60, origin: 0, dest: 11, tag: "S1_dest_11h_D2_th60" },

  #           # === Ã‰tude 1 : Origine + destination
  #           { ds: "D2", th: 60, origin: 7, dest: 7, tag: "S1_origin7h_dest7h_D2_th60" },

  #           # === Ã‰tude 2 : variation du seuil th
  #           { ds: "D2", th: 15, origin: 7, dest: 7, tag: "S2_D2_th15_origin7h_dest7h" },
  #           { ds: "D2", th: 30, origin: 7, dest: 7, tag: "S2_D2_th30_origin7h_dest7h" },
  #           { ds: "D2", th: 45, origin: 7, dest: 7, tag: "S2_D2_th45_origin7h_dest7h" },
  #           { ds: "D2", th: 60, origin: 7, dest: 7, tag: "S2_D2_th60_origin7h_dest7h" },
  #           { ds: "D2", th: 90, origin: 7, dest: 7, tag: "S2_D2_th90_origin7h_dest7h" },

  #           # === Ã‰tude 3 : variation du dataset
  #           { ds: "D1", th: 60, origin: 7, dest: 7, tag: "S3_D1_th60_origin7h_dest7h" },
  #           { ds: "D3", th: 60, origin: 7, dest: 7, tag: "S3_D3_th60_origin7h_dest7h" },
  #           { ds: "D4", th: 60, origin: 7, dest: 7, tag: "S3_D4_th60_origin7h_dest7h" }
  #         ]

  #   steps:
  #     - uses: actions/checkout@v4

  #     - name: Download dataset
  #       uses: actions/download-artifact@v4
  #       with:
  #         name: dataset
  #         path: /app/data/

  #     - name: Run full pipeline (matrix experiment)
  #       run: |
  #         echo "ðŸš€ Running full pipeline experiment: ${{ matrix.exp.tag }}"
  #         ./run-app.sh \
  #           --stage=all --local \
  #           --ds=${{ matrix.exp.ds }} \
  #           --th=${{ matrix.exp.th }} \
  #           --originHours=${{ matrix.exp.origin }} \
  #           --destHours=${{ matrix.exp.dest }} \
  #           --tag=${{ matrix.exp.tag }}

  #     - name: Upload End-to-End analysis
  #       uses: actions/upload-artifact@v4
  #       with:
  #         name: analysis-${{ matrix.exp.tag }}
  #         path: analysis/

  # ============================================
  # Ã‰tape 4 â€” Silver Stage
  # ============================================
  silver:
    name: âšª Silver - Cleaning & Standardization
    needs: bronze
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/auduvignac/flight:latest
    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v4

      - name: Download dataset
        uses: actions/download-artifact@v4
        with:
          name: dataset
          path: /app/data/

      - name: Download Delta Bronze
        uses: actions/download-artifact@v4
        with:
          name: delta-bronze
          path: /app/delta/

      - name: Run Silver stage
        run: ./run-app.sh --stage=silver --local --persist

      - name: Upload Silver analysis
        uses: actions/upload-artifact@v4
        with:
          name: analysis-silver
          path: analysis/

      - name: Upload Delta Silver
        uses: actions/upload-artifact@v4
        with:
          name: delta-silver
          path: /app/delta/

  # ============================================
  # Ã‰tape 5 â€” Gold Stage
  # ============================================
  gold:
    name: ðŸŸ¡ Gold - Build Joint Tables
    needs: silver
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/auduvignac/flight:latest
    timeout-minutes: 360
    strategy:
      matrix:
        th: [15, 30, 45, 60, 90]

    steps:
      - uses: actions/checkout@v4

      - name: Download dataset
        uses: actions/download-artifact@v4
        with:
          name: dataset
          path: /app/data/

      - name: Download Delta Silver
        uses: actions/download-artifact@v4
        with:
          name: delta-silver
          path: /app/delta/

      - name: Build Gold for threshold
        run: |
          echo "Building Gold for threshold ${{ matrix.th }} minutes"
          ./run-app.sh \
            --stage=gold --local --persist \
            --deltaBase="/app/delta" \
            --th=${{ matrix.th }}

      - name: Upload Delta Gold
        uses: actions/upload-artifact@v4
        with:
          name: delta-gold-th${{ matrix.th }}
          path: /app/delta/gold/


  # ============================================
  # Ã‰tape 6 â€” ML Stage
  # ============================================
  ml:
    name: ðŸ§  ML - Feature Engineering & Modeling
    needs: gold
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/auduvignac/flight:latest
    timeout-minutes: 60

    strategy:
      fail-fast: false
      matrix:
        exp:
          [
            # === Baseline
            { ds: "D2", th: 60, origin: 0, dest: 0, tag: "Baseline_D2_th60_noWeather" },

            # === Ã‰tude 1 : Origine seule
            { ds: "D2", th: 60, origin: 1, dest: 0, tag: "S1_origin_1h_D2_th60" },
            { ds: "D2", th: 60, origin: 3, dest: 0, tag: "S1_origin_3h_D2_th60" },
            { ds: "D2", th: 60, origin: 5, dest: 0, tag: "S1_origin_5h_D2_th60" },
            { ds: "D2", th: 60, origin: 7, dest: 0, tag: "S1_origin_7h_D2_th60" },
            { ds: "D2", th: 60, origin: 9, dest: 0, tag: "S1_origin_9h_D2_th60" },
            { ds: "D2", th: 60, origin: 11, dest: 0, tag: "S1_origin_11h_D2_th60" },

            # === Ã‰tude 1 : Destination seule
            { ds: "D2", th: 60, origin: 0, dest: 1, tag: "S1_dest_1h_D2_th60" },
            { ds: "D2", th: 60, origin: 0, dest: 3, tag: "S1_dest_3h_D2_th60" },
            { ds: "D2", th: 60, origin: 0, dest: 5, tag: "S1_dest_5h_D2_th60" },
            { ds: "D2", th: 60, origin: 0, dest: 7, tag: "S1_dest_7h_D2_th60" },
            { ds: "D2", th: 60, origin: 0, dest: 9, tag: "S1_dest_9h_D2_th60" },
            { ds: "D2", th: 60, origin: 0, dest: 11, tag: "S1_dest_11h_D2_th60" },

            # === Ã‰tude 1 : Origine + destination
            { ds: "D2", th: 60, origin: 7, dest: 7, tag: "S1_origin7h_dest7h_D2_th60" },

            # === Ã‰tude 2 : variation du seuil th
            { ds: "D2", th: 15, origin: 7, dest: 7, tag: "S2_D2_th15_origin7h_dest7h" },
            { ds: "D2", th: 30, origin: 7, dest: 7, tag: "S2_D2_th30_origin7h_dest7h" },
            { ds: "D2", th: 45, origin: 7, dest: 7, tag: "S2_D2_th45_origin7h_dest7h" },
            { ds: "D2", th: 60, origin: 7, dest: 7, tag: "S2_D2_th60_origin7h_dest7h" },
            { ds: "D2", th: 90, origin: 7, dest: 7, tag: "S2_D2_th90_origin7h_dest7h" },

            # === Ã‰tude 3 : variation du dataset
            { ds: "D1", th: 60, origin: 7, dest: 7, tag: "S3_D1_th60_origin7h_dest7h" },
            { ds: "D3", th: 60, origin: 7, dest: 7, tag: "S3_D3_th60_origin7h_dest7h" },
            { ds: "D4", th: 60, origin: 7, dest: 7, tag: "S3_D4_th60_origin7h_dest7h" }
          ]

    steps:
      - uses: actions/checkout@v4

      - name: Download dataset
        uses: actions/download-artifact@v4
        with:
          name: dataset
          path: /app/data/

      - name: Download matching Delta Gold
        uses: actions/download-artifact@v4
        with:
          name: delta-gold-th${{ matrix.exp.th }}
          path: /app/delta-tmp/

      - name: Prepare isolated Delta directory
        run: |
          DELTA_ISO="/app/delta-${{ matrix.exp.tag }}"
          mkdir -p "$DELTA_ISO/gold"
          cp -r /app/delta-tmp/* "$DELTA_ISO/gold/"

      - name: Run ML stage (matrix experiment)
        run: |
          echo "ðŸš€ Running ML experiment: ${{ matrix.exp.tag }}"
          ./run-app.sh \
            --stage=ml --local --persist \
            --deltaBase="/app/delta-${{ matrix.exp.tag }}" \
            --ds=${{ matrix.exp.ds }} \
            --th=${{ matrix.exp.th }} \
            --originHours=${{ matrix.exp.origin }} \
            --destHours=${{ matrix.exp.dest }} \
            --tag=${{ matrix.exp.tag }}

      - name: Upload ML analysis
        uses: actions/upload-artifact@v4
        with:
          name: analysis-${{ matrix.exp.tag }}
          path: analysis/

      #
      # EXPORT + UPLOAD ML METRICS / FEATURE IMPORTANCES
      # (version robuste sans erreur 409)
      #
      - name: Prepare ML export directory
        run: |
          EXP_DIR="ml_export_${{ matrix.exp.tag }}"
          mkdir -p "$EXP_DIR"

          cp /app/delta/analysis/ml/*.json "$EXP_DIR/" 2>/dev/null || true
          cp /app/delta/analysis/ml/*.csv "$EXP_DIR/" 2>/dev/null || true
          cp /app/delta/analysis/ml/*.png "$EXP_DIR/" 2>/dev/null || true
          cp /app/delta/analysis/ml/*.html "$EXP_DIR/" 2>/dev/null || true

          echo "ðŸ“¦ Files collected for export ($EXP_DIR):"
          ls -R "$EXP_DIR"

      - name: Upload ML results (unique artifact)
        uses: actions/upload-artifact@v4
        with:
          name: ml-results-${{ matrix.exp.tag }}
          path: ml_export_${{ matrix.exp.tag }}



  # ============================================
  # Ã‰tape 7 â€” AGRÃ‰GATION DES RÃ‰SULTATS ML
  # ============================================
  aggregate-report:
    name: ðŸ“Š Aggregate ML Results (Global Report)
    needs: ml
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Install jq (required for parsing JSON)
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Download all ML result artifacts
        run: |
          mkdir -p ml_artifacts

          echo "ðŸ” Fetching artifact list from GitHub API..."
          curl -s \
            -H "Authorization: Bearer $GH_TOKEN" \
            -H "Accept: application/vnd.github+json" \
            https://api.github.com/repos/${{ github.repository }}/actions/runs/${{ github.run_id }}/artifacts \
            > artifacts.json

          echo "ðŸ“„ List of artifacts:"
          cat artifacts.json

          echo "â¬‡ï¸ Extracting ml-results-* artifacts..."
          ARTIFACT_IDS=$(jq -r '.artifacts[] | select(.name | startswith("ml-results-")) | "\(.id):\(.name)"' artifacts.json)

          if [ -z "$ARTIFACT_IDS" ]; then
            echo "âŒ No ml-results-* artifacts found!"
            exit 1
          fi

          echo "$ARTIFACT_IDS" | while IFS=":" read -r ID NAME; do
            echo "ðŸ“¦ Downloading artifact $NAME (ID: $ID)"
            curl -L -o "$NAME.zip" \
              -H "Authorization: Bearer $GH_TOKEN" \
              "https://api.github.com/repos/${{ github.repository }}/actions/artifacts/$ID/zip"

            mkdir -p "ml_artifacts/$NAME"
            unzip -qo "$NAME.zip" -d "ml_artifacts/$NAME"
          done
        env:
          GH_TOKEN: ${{ github.token }}

      - name: List downloaded ML results
        run: |
          echo "ðŸ“‚ Downloaded ML results:"
          ls -R ml_artifacts || echo "âš ï¸ No files found"

      - name: Install Python dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip install pandas jinja2 plotly

      - name: Generate Global ML Report
        run: |
          python .github/scripts/generate_global_report.py \
            --input ml_artifacts \
            --output ml_report_all.html \
            --template templates/ml_report_global_template.html

      - name: Upload Global ML Report
        uses: actions/upload-artifact@v4
        with:
          name: ml-report-global
          path: ml_report_all.html

      - name: Add Summary Info
        run: |
          echo "## Global ML Report generated ðŸŽ‰" >> $GITHUB_STEP_SUMMARY
          echo "Download artifact: **ml-report-global**" >> $GITHUB_STEP_SUMMARY


  # ============================================
  # Ã‰tape 8 â€” Publication GitHub Pages
  # ============================================
  publish-pages:
    name: ðŸŒ Publish ML Global Report to GitHub Pages
    needs: aggregate-report
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pages: write
      id-token: write

    steps:
      - uses: actions/checkout@v4

      - name: Download global report artifact
        uses: actions/download-artifact@v4
        with:
          name: ml-report-global
          path: public

      - name: Disable Jekyll
        run: echo "" > public/.nojekyll

      - name: Configure Pages
        uses: actions/configure-pages@v4

      - name: Upload artifact to Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: public

      - name: Deploy to GitHub Pages
        uses: actions/deploy-pages@v4