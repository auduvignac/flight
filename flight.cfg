# ===========================================================
# üõ´ CONFIGURATION FLIGHT PROJECT
# ===========================================================

app {
  # Environnement : "Local" ou "Hadoop"
  env = "Local"

  # === INPUTS LOCAUX ===
  input {
    flights.dir = "./Data/flights"       # dossier local ou HDFS
    weather.dir = "./Data/weather"
    mapping     = "./Data/mapping.csv"

    # Mois de donn√©es utilis√©s (utile pour le filtrage)
    months_f = ["2013-11"]
    months_w = ["2013-11"]
  }

  # === INPUTS HADOOP (optionnel, si tu ex√©cutes sur cluster) ===
  hadoop {
    input {
      flights.dir = "/user/flight/input/flights"
      weather.dir = "/user/flight/input/weather"
      mapping     = "/user/flight/input/mapping.csv"
    }

    output {
      delta.base {
        bronze = "/user/flight/output/bronze"
        silver = "/user/flight/output/silver"
        gold   = "/user/flight/output/gold"
      }
    }
  }

  # === OUTPUTS LOCAUX ===
  output {
    delta.base {
      bronze = "./output/bronze"
      silver = "./output/silver"
      gold   = "./output/gold"
    }
  }

  # === PARAM√àTRES M√âTIER ===
  params {
    thMinutes = 60                      # fen√™tre de jointure en minutes
    missingness.threshold = 0.6         # seuil de valeurs manquantes
  }
}

# ===========================================================
# ‚öôÔ∏è CONFIGURATION SPARK
# ===========================================================
spark {
  # Master local[*] ou spark://spark-master:7077
  master = "spark://spark-master:7077"

  appName = "FlightDelayPrediction"

  # Extensions Delta Lake
  sql.extensions = "io.delta.sql.DeltaSparkSessionExtension"
  sql.catalog    = "spark_catalog"

  # Configurations Spark personnalis√©es
  conf {
    "spark.sql.shuffle.partitions" = "200"
    "spark.driver.memory"          = "2g"
    "spark.executor.memory"        = "2g"
    "spark.sql.debug.maxToStringFields" = "200"
  }
}